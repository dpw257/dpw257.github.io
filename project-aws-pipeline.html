<!DOCTYPE HTML>
<html>
	<head>
		<title>AWS Data Pipeline - Data Engineering</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="right-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Inner -->
						<div class="inner">
							<header>
								<h1><a href="https://dpw257.github.io" id="logo">Data Engineering</a></h1>
							</header>
						</div>



				</div>

			<!-- Main -->
				<div class="wrapper style1">

					<div class="container">
						<div class="row gtr-200">
							<div class="col-8 col-12-mobile" id="content">
								<article id="main">
									<header>
										<h2>AWS Data Pipeline for User Upload Data</h2>
										<p>

										</p>
									</header>
									<p><b><i>
										A data pipeline was created based on the model currently used by Pinterest, the leading image sharing platform, which 
										has 500 million active users who generate billions of data points daily. 
									</b></p></i>
									<p><b><i>
										The company continually trials improvements to its service, so monitoring site traffic in real-time is vital for quick 
										decision-making and problem-solving. User upload data must also be stored so that it can be retrieved 
										and processed at any time.
									</b></p></i>
									<p><b><i>
										To handle such large amounts of data, Pinterest developed a powerful experimentation pipeline capable of processing 
										batch and stream data in parallel, as well as storing historical data and scheduling daily batch tasks.
									</b></p></i>
									<section>
										<header>
											<h3>1. Project objectives</h3>
										</header>
										<p>
											Create an end-to-end data pipeline based on the Pinterest model using the AWS Cloud that will:<br>
											&nbsp; &#x25cf; &nbsp; Employ a Lambda architecture for batch and streaming data.<br>
											&nbsp; &#x25cf; &nbsp; Send data to Databricks for analysis and to be saved as a Delta table.<br>
											&nbsp; &#x25cf; &nbsp; Store batch data in an AWS S3 bucket.<br>
											&nbsp; &#x25cf; &nbsp; Schedule an Airflow DAG to trigger a Databricks notebook to run daily.<br>
											</p><br>
									</section>
									
									<section>
										<header>
										<h3>2. Pipeline design</h3><br>
										</header>
										<h1>2.1. User uploads</h1><br>
										<p>In the original Pinterest pipeline, posts are uploaded by users to a RESTful API. To represent the POST requests sent by users, 
											a RDS database with three tables of data (post data, geolocation data, user data) was stored on the AWS cloud.</p><br>

										<h1>2.2. Stream processing</h1><br>
										<p>The RESTful API was configured to allow streams to be created in AWS Kinesis and records to be added to the streams. 
											Three streams were then created, one for each data table. Requests were sent to the API using a Python script, to add 
											records from the three Pinterest tables to their corresponding Kinesis streams.</p><br>
										<p><div class="scroll-container">
											<img src="images/project-aws-pipeline1.jpg" alt="" style="width:720px;">
										  </div>
										<span style="font-size:10pt"><i>Systems diagram of the data pipeline</i></span></p><br>
										<h1>2.3. Batch processing</h1><br>
										<p>An EC2 instance was created on the AWS Cloud to host the client for the MSK cluster. A Kafka REST Proxy was created on the 
											AWS API Gateway console and set up on the EC2 instance, enabling data to be sent to the MSK cluster and saved in three topics 
											in an S3 bucket using a Python script and an MSK plugin-connector pair.</p><br>
										<p><div class="scroll-container">
											<img src="images/project-aws-pipeline2.jpg" alt="" style="width:700px;">
										  </div>
										<span style="font-size:10pt"><i>Configuration of MSK connector from cluster to S3 bucket</i></span></p><br>
										
										<h1>2.4. Daily batch workloads</h1><br>
										<p>To automate the process of processing batch data, a Databricks notebook was orchestrated to run daily using 
											a DAG script uploaded to MWAA (Airflow).</p><br>
										<p><div class="scroll-container">
											<img src="images/project-aws-pipeline3.jpg" alt="" style="width:700px;">
										  </div>
										<span style="font-size:10pt"><i>DAG running on Amazon Managed Apache Airflow</i></span></p><br>
										
										<h1>2.5. Transforming, cleaning and querying data</h1><br>
										<p>On the Databricks platform, Spark was used to transform, clean, aggregate and analyse the data stored in the S3 data lake.</p><br>
										<p><div class="scroll-container">
											<img src="images/project-aws-pipeline4.jpg" alt="" style="width:700px;">
										  </div>
										<span style="font-size:10pt"><i>Spark SQL query run in Databricks</i></span></p><br>
									</section>
									<section>
										<header>
											<h3>3. Results and insights</h3>
										</header>
										<p>
											30,000 rows of streaming and batch data were sent to Databricks to be analysed, with the DAG script successfully 
											ochestrating daily processing.<br><br>
											The data pipeline offers good scalability with the AWS ecosystem providing storage and compute capabilities to meet 
											demand as needed.<br><br>
											A data dashboard could be added as a final stage to provide analysts with a real-time overview of user traffic.
										</p>
									</section>
									
									<section>
										<footer>
											<br><br>
											<a href="https://dpw257.github.io" class="button">Home</a>
										</footer>
									</section>
								</article>
							</div>
							<div class="col-4 col-12-mobile" id="sidebar">
								<hr class="first" />
								<section>
									<p>
										Explore the code for this project:
									</p>
									<header>
										<h3><a href="https://github.com/dpw257/aws_data_pipeline_image_sharing" class="icon brands fa-github">   GitHub repository</a></h3>
									</header>

								</section>
								<hr />
								<section>
									<header>
										<h3><a>Other projects</a></h3>
									</header>

									<div class="row gtr-50">
										<!--  <div class="col-4">
											<a href="project-aws-pipeline" class="image fit"><img src="images/pic10.jpg" alt="" /></a>
										</div>
										<div class="col-8">
											<h4><a href="project-aws-pipeline">AWS data pipeline for user data</a></h4>
											<p>
												Batch and stream processing,<br>AWS Kinesis, S3, Databricks, Airflow.
											</p>
										</div>  -->
										<div class="col-4">
											<a href="project-sql-upload-data" class="image fit"><img src="images/pic11.jpg" alt="" /></a>
										</div>
										<div class="col-8">
											<h4><a href="project-sql-upload-data">PostgreSQL database for sales</a></h4>
											<p>
												Data extracted from RDS database and RESTful API, cleaned, SQL.
											</p>
										</div>
										<div class="col-4">
											<a href="project-product-recommend" class="image fit"><img src="images/pic12.jpg" alt="" /></a>
										</div>
										<div class="col-8">
											<h4><a href="project-product-recommend">Product recommendation system</a></h4>
											<p>
												BeautifulSoup, product clustering, EDA, dynamic pricing.
											</p>
										</div>
										<div class="col-4">
											<a href="project-pos-tagger" class="image fit"><img src="images/pic13.jpg" alt="" /></a>
										</div>
										<div class="col-8">
											<h4><a href="project-pos-tagger">POS tagger for minority language</a></h4>
											<p>
												Based on hidden Markov model with Viterbi algorithm.</p>
											</p>
										</div>
										<!--<div class="col-4">
											<a href="#" class="image fit"><img src="images/pic14.jpg" alt="" /></a>
										</div>
										<div class="col-8">
											<h4>Malesuada fermentum</h4>
											<p>
												Amet nullam fringilla nibh nulla convallis tique ante proin.
											</p>
										</div>-->
									</div>
								</section>
								<hr />
							</div>
						</div>
						
					</div>

				</div>

			<!-- Footer -->
				<div id="footer">
					<div class="container">
						<div class="row">
							<div class="col-12">

								<!-- Contact -->


								<!-- Copyright -->
									<div class="copyright">
										<ul class="menu">
											<li>&copy; Daniel White. All rights reserved.</li><li>Design: HTML5 UP</li>
										</ul>
									</div>

							</div>

						</div>
					</div>
				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
